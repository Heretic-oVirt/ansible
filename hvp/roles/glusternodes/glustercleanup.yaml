---
# Ansible playbook to completely revert GlusterFS nodes configuration (cleanup)
# Note: the aim is to undo any effect of running the glusternodes.yaml and storageservices.yaml playbooks
- name: Generate SSH key if not present
  hosts: localhost
  tasks:
    - include: ../common/tasks/createkeys.yaml
- name: Inspect Gluster nodes
  hosts: glusternodes
  remote_user: root
  tasks:
    - include: ../common/tasks/setupkeys.yaml
# TODO: write a suitable hvp_gluster_disks fact-finding module to drive the LVM-cleanup part in gdeploy-cleanup.j2
# Note: as a temporary workaround we use an external helper script lvm-cleanup.sh
#    - name: Gather Gluster-used disks
#      hvp_gluster_disks: include_ssd=1
- name: Delete Gluster-block based volumes
  hosts: gluster_master
  remote_user: root
  tasks:
    - name: Get common vars
      include_vars:
        file: ../common/vars/hvp.yaml
    - name: Enumerate Gluster-block based iSCSI LUNs
      command: "gluster-block list {{ hvp_blockshare_volume_name }}"
      ignore_errors: yes
      register: enumerateluns_result
    - name: Delete all iSCSI LUNs found
      command: "gluster-block delete {{ hvp_blockshare_volume_name }}/{{ item }}"
      ignore_errors: yes
      register: deleteluns_result
      with_items:
        - "{{ enumerateluns_result.stdout_lines }}"
      when:
        - enumerateluns_result.rc == 0
- name: Stop and unconfigure Gluster-block
  hosts: glusternodes
  remote_user: root
  tasks:
    - name: Get common vars
      include_vars:
        file: ../common/vars/hvp.yaml
    - name: Disable and stop the Gluster-block service
      systemd:
        name: gluster-blockd
        enabled: False
        state: stopped
        no_block: no
      ignore_errors: yes
- name: Stop and unconfigure CTDB
  hosts: glusternodes
  remote_user: root
  tasks:
    - name: Get common vars
      include_vars:
        file: ../common/vars/hvp.yaml
    - name: Disable and stop the CTDB service
      systemd:
        name: ctdb
        enabled: False
        state: stopped
        no_block: no
    - name: Disable and stop the RT bandwidth configuration
      systemd:
        name: cgroup-rt-bandwidth
        enabled: False
        state: stopped
        no_block: no
    - name: Disable and stop the wait service for Gluster ctdb volume
      systemd:
        name: gluster-ctdb-wait-online
        enabled: False
        state: stopped
        no_block: no
    - name: Disable and stop the local mounting of the Gluster ctdb volume
      systemd:
        name: gluster-lock.mount
        enabled: False
        state: stopped
        no_block: no
    - name: Copy the LVM cleanup helper script
      # TODO: remove when properly implemented by means of fact finding above and gDeploy below
      copy:
        src: lvm-cleanup.sh
        dest: /usr/local/sbin/lvm-cleanup.sh
        owner: root
        group: root
        mode: 0755
- name: Perform gDeploy-based Gluster unconfiguration
  hosts: localhost
  remote_user: root
  tasks:
    - name: Get common vars
      include_vars:
        file: ../common/vars/hvp.yaml
    - name: Prepare gDeploy cleanup configuration file
      template:
        src: templates/gdeploy-cleanup.j2
        dest: "{{ playbook_dir }}/gdeploy-cleanup.conf"
        owner: root
        group: root
        mode: 0644
    - name: Perform actual gDeploy-based Gluster unconfiguration
      command: gdeploy -k -vv -c "{{ playbook_dir }}/gdeploy-cleanup.conf"
      register: gdeploy_cleanup_result
- name: Perform LVM cleanup on nodes
  # TODO: remove when properly implemented by means of fact-finding and gDeploy above
  hosts: glusternodes
  remote_user: root
  tasks:
    - name: Get common vars
      include_vars:
        file: ../common/vars/hvp.yaml
    - name: Execute the LVM cleanup helper script
      vars:
        ansible_ssh_pipelining: no
      command: /usr/local/sbin/lvm-cleanup.sh
      register: lvmcleanup_result
...
