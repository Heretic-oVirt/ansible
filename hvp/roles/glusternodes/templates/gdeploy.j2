# gDeploy Gluster configuration file for HVP

# Nodes in the trusted pool
# TODO: dynamically choose the weakest node (less disk space / slower CPU) as the arbiter-only node (listed last)
[hosts]
{% for host in groups['glusternodes'] %}
{{ host }}
{% endfor %}

# Do a sanity check before proceding
# Note: No need to test disks (with "-d sdb,sdc...") since we get them from a fact-gathering custom Ansible module that already skips invalid ones
# TODO: Check disabled since it consistently hangs indefinitely during last pass - find out why and enable again
#[script1]
#action=execute
#file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -h {{ groups['glusternodes'] | join(',') }}
#ignore_script_errors=no

# Blacklist devices in multipath.conf
[script2]
action=execute
file=/usr/share/gdeploy/scripts/blacklist_all_disks.sh
ignore_script_errors=no

# Disable hooks
[script3]
action=execute
file=/usr/share/gdeploy/scripts/disable-gluster-hooks.sh

# SELinux is active
[selinux]
yes

# Disk configuration hints
# Note: disktype can be omitted too (default values used)
# TODO: generalize autodetecting hardware/software RAID setups
#[disktype]
#jbod


# Setup LVM
# TODO: allow for SSD-backed (to be autodetected) LVM caches (either only one SSD partitioned to proportionally provide cache to all vgs or one dedicated SSD cache for each vg)
# TODO: give error for an insufficient number of disks/hosts or insufficient space on disks
# Note: enginedomain (thick {{ hvp_enginedomain_size }}) LVs will host the oVirt Engine Gluster volume bricks
# Note: vmstoredomain (thin {{ hvp_vmstoredomain_size }}) LVs will host the oVirt vm disks Gluster volume bricks
# Note: isodomain (thin {{ hvp_isodomain_size }}) LVs will host the oVirt ISO images Gluster volume bricks
# Note: ctdb (thick {{ hvp_ctdb_size }} - no arbiter) LVs will host the CTDB lock Gluster volume bricks
# Note: winshare (thin {{ hvp_winshare_size }}) LVs will host the Samba share Gluster volume bricks
# Note: unixshare (thin {{ hvp_unixshare_size }}) LVs will host the Ganesha-NFS (currently Gluster-NFS) share Gluster volume bricks
# Note: blockshare (thin {{ hvp_blockshare_size }}) LVs will host the Gluster-Block share Gluster volume bricks
# Note: backup (thin {{ hvp_backup_size }}) LVs will host the Bareos Gluster volume bricks
# Note:
#  every suitable disk used will be made into the single PV of a VG
#  if we have only three nodes than one node (the last one) is meant to be arbiter-only
#  the arbiter-only node will use only one suitable disk/PV/VG (the first smallest) for all LVs
#  find the minimum number of suitable disks-per-node available among all active (non arbiter-only) nodes
#   if it is one then all LVs will be in one single VG on that single PV disk
#   if it is two then all oVirt LVs will be in one VG (PV: the last smallest disk) and the CTDB/share/backup LVs on the other VG (PV: the first bigger disk)
#   if it is three then CTDB/share/backup LVs will be in one VG (PV: the first biggest disk), the enginedomain/isodomain LVs on another VG (PV: the last smallest disk) and the vmstoredomain LV will be in the other VG (PV: the first remaining disk excluding those already selected)
#  suitable disks exceeding the number of three-per-node will be left unused (but which disks get used will be decided according to the rules detailed above)

{% if groups['glusternodes'] | length == 3 %}

# Three nodes detected: simple replica-3 replicated-with-fixed-arbiter volumes

# Active nodes setup

{% set disk_count = [ hostvars[groups['glusternodes'][0]]['hvp_free_disks'] | length, hostvars[groups['glusternodes'][1]]['hvp_free_disks'] | length ] | min %}

{% if disk_count | int == 1 %}
# Only one suitable disk
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
{% set node_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ node_disk }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ node_disk }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int == 2 %}
# Two suitable disks found
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
# Note: we will use the smallest for oVirt
{% set ovirt_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_disk }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_disk }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_disk }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int >= 3 %}
# Three or more suitable disks found
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the first biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
# Note: we will use the last smallest for oVirt Engine and ISO
{% set ovirt_other_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}
# Note: we will use the first remaining (excluding those already selected) for oVirt virtual machines
{% set ovirt_vm_disk = suitable_disks | map(attribute='name') | difference([ ovirt_other_disk, share_disk ]) | list | sort | first %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_other_disk }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
ignore_pv_errors=no

[pv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_vm_disk }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_other_disk }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_disk }}
ignore_vg_errors=no

[vg{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
pvname={{ ovirt_vm_disk }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}11:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% endif %}

# Arbiter node setup

{% set arbiter_disk = hostvars[groups['glusternodes'][2]]['hvp_free_disks'] | selectattr('size', 'equalto', hostvars[groups['glusternodes'][2]]['hvp_free_disks'] | map(attribute='size') | min) | map(attribute='name') | list | sort | first %}

[pv21:{{ groups['glusternodes'][2] }}]
action=create
devices={{ arbiter_disk }}
ignore_pv_errors=no

[vg21:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
pvname={{ arbiter_disk }}
ignore_vg_errors=no

[lv21:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv22:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv23:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
virtualsize={{ hvp_enginedomain_arbitersize }}
ignore_lv_errors=no

[lv24:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_arbitersize }}
ignore_lv_errors=no

[lv25:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_arbitersize }}
ignore_lv_errors=no

[lv26:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_arbitersize }}
ignore_lv_errors=no

[lv27:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_arbitersize }}
ignore_lv_errors=no

[lv28:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_arbitersize }}
ignore_lv_errors=no

[lv29:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_arbitersize }}
ignore_lv_errors=no

{% else %}
# TODO: add support for more than 3 nodes (rotating arbiter - replicated+distributed volumes with a multiple of 3 disks on each node)
# TODO: assume that 3 free disks for each node are equivalent to one disk in the fixed-arbiter configuration above
# TODO: the following will intentionally cause an error - remove when adding complete logic
NO ARBITER-ONLY NODE!!!
{% endif %}

[service1]
action=enable
service=chronyd

[service2]
action=restart
service=chronyd

[service3]
action=restart
service=glusterd
slice_setup=yes

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp,24010/tcp,3260/tcp
services=glusterfs

# Gluster volume definitions
# TODO: add support for more than 3 nodes (replicated+distributed volumes)
# Note: shard block size lowered from 512 MiB to 64 MiB as per https://bugzilla.redhat.com/show_bug.cgi?id=1468969

[volume1]
action=create
volname=enginedomain
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=virt,36,36,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/enginedomain/brick1/brick
ignore_volume_errors=no

[volume2]
action=create
volname=vmstoredomain
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=virt,36,36,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/vmstoredomain/brick1/brick
ignore_volume_errors=no

[volume3]
action=create
volname=isodomain
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
# TODO: use NFS-Ganesha as soon as it is available - using internal Gluster-NFS meanwhile
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,nfs.disable
value=virt,36,36,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on,off
brick_dirs=/gluster_bricks/isodomain/brick1/brick
ignore_volume_errors=no

[volume4]
action=create
volname=ctdb
transport=tcp
replica=yes
replica_count=3
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=virt,0,0,off,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/ctdb/brick1/brick
ignore_volume_errors=no

[volume5]
action=create
volname=winshare
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/winshare/brick1/brick
ignore_volume_errors=no

[volume6]
action=create
volname=unixshare
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
# TODO: use NFS-Ganesha as soon as it is available - using internal Gluster-NFS meanwhile
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,nfs.disable
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on,off
brick_dirs=/gluster_bricks/unixshare/brick1/brick
ignore_volume_errors=no

[volume7]
action=create
volname=blockshare
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/blockshare/brick1/brick
ignore_volume_errors=no

[volume8]
action=create
volname=backup
transport=tcp
replica=yes
replica_count=3
arbiter_count=1
force=yes
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,8,10,off,on,off,on,on
brick_dirs=/gluster_bricks/backup/brick1/brick
ignore_volume_errors=no
