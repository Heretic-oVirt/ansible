# gDeploy Gluster configuration file for HVP

# Nodes in the trusted pool
# TODO: dynamically choose the weakest node (less disk space / slower CPU) as the arbiter-only node (listed last)
[hosts]
{% for host in groups['glusternodes'] %}
{{ host }}
{% endfor %}

# Do a sanity check before proceding
# Note: No need to test disks (with "-d sdb,sdc...") since we get them from a fact-gathering custom Ansible module that already skips invalid ones
# TODO: Check disabled since it consistently hangs indefinitely during last pass - find out why and enable again
#[script1]
#action=execute
#file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -h {{ groups['glusternodes'] | join(',') }}
#ignore_script_errors=no

# Blacklist devices in multipath.conf
[script2]
action=execute
file=/usr/share/gdeploy/scripts/blacklist_all_disks.sh
ignore_script_errors=no

# Disable hooks
[script3]
action=execute
file=/usr/share/gdeploy/scripts/disable-gluster-hooks.sh

# SELinux is active
[selinux]
yes

# Disk configuration hints
# Note: disktype can be omitted too (default values used)
# TODO: generalize autodetecting hardware/software RAID setups
#[disktype]
#jbod


# Setup LVM
# TODO: allow for SSD-backed (to be autodetected) LVM caches (either only one SSD partitioned to proportionally provide cache to all vgs or one dedicated SSD cache for each vg)
# TODO: give error for an insufficient number of disks/hosts or insufficient space on disks
# TODO: calculate all sizes from actually available disk space
# Note: enginedomain (thick {{ hvp_enginedomain_size }}) LVs will host the oVirt Engine Gluster volume bricks
# Note: vmstoredomain (thin {{ hvp_vmstoredomain_size }}) LVs will host the oVirt vm disks Gluster volume bricks
# Note: isodomain (thin {{ hvp_isodomain_size }}) LVs will host the oVirt ISO images Gluster volume bricks
# Note: ctdb (thick {{ hvp_ctdb_size }} - no arbiter) LVs will host the CTDB lock Gluster volume bricks
# Note: winshare (thin {{ hvp_winshare_size }}) LVs will host the Samba share Gluster volume bricks
# Note: unixshare (thin {{ hvp_unixshare_size }}) LVs will host the Ganesha-NFS (currently Gluster-NFS) share Gluster volume bricks
# Note: blockshare (thin {{ hvp_blockshare_size }}) LVs will host the Gluster-Block share Gluster volume bricks
# Note: backup (thin {{ hvp_backup_size }}) LVs will host the Bareos Gluster volume bricks
# Note:
#  every suitable disk used will be made into the single PV of a VG
#  if we have only one node than it will work in replica-1
#  if we have only three nodes than one node (the last one) is meant to be arbiter-only
#  the arbiter-only node will use only one suitable disk/PV/VG (the first smallest) for all LVs
#  find the minimum number of suitable disks-per-node available among all active (non arbiter-only) nodes
#   if it is one then all LVs will be in one single VG on that single PV disk
#   if it is two then all oVirt LVs will be in one VG (PV: the last smallest disk) and the CTDB/share/backup LVs on the other VG (PV: the first bigger disk)
#   if it is three then CTDB/share/backup LVs will be in one VG (PV: the first biggest disk), the enginedomain/isodomain LVs on another VG (PV: the last smallest disk) and the vmstoredomain LV will be in the other VG (PV: the first remaining disk excluding those already selected)
#  suitable disks exceeding the number of three-per-node will be left unused (but which disks get used will be decided according to the rules detailed above)

{% if groups['glusternodes'] | length == 1 %}

# Single node detected: simple replica-1 volumes

# Single node setup

{% set disk_count = hostvars[groups['glusternodes'][0]]['hvp_free_disks'] | length %}

{% if disk_count | int == 1 %}
# Only one suitable disk
{% for node_index in range(1) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
{% set node_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set pv_name = hvp_use_vdo | ternary('mapper/vdo_' + node_disk, node_disk) %}

{% if hvp_use_vdo %}
{% set vdo_name = ('vdo_' + node_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ node_disk }}
names={{ vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int == 2 %}
# Two suitable disks found
{% for node_index in range(1) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set share_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + share_disk, share_disk) %}
# Note: we will use the smallest for oVirt
{% set ovirt_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}
{% set ovirt_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_disk, ovirt_disk) %}

{% if hvp_use_vdo %}
{% set share_vdo_name = ('vdo_' + share_disk) %}
{% set ovirt_vdo_name = ('vdo_' + ovirt_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_disk }}
names={{ ovirt_vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
names={{ share_vdo_name }}
logicalsize={{ (((hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int >= 3 %}
# Three or more suitable disks found
{% for node_index in range(1) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the first biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set share_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + share_disk, share_disk) %}
# Note: we will use the last smallest for oVirt Engine and ISO
{% set ovirt_other_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}
{% set ovirt_other_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_other_disk, ovirt_other_disk) %}
# Note: we will use the first remaining (excluding those already selected) for oVirt virtual machines
{% set ovirt_vm_disk = suitable_disks | map(attribute='name') | difference([ ovirt_other_disk, share_disk ]) | list | sort | first %}
{% set ovirt_vm_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_vm_disk, ovirt_vm_disk) %}

{% if hvp_use_vdo %}
{% set share_vdo_name = ('vdo_' + share_disk) %}
{% set ovirt_other_vdo_name = ('vdo_' + ovirt_other_disk) %}
{% set ovirt_vm_vdo_name = ('vdo_' + ovirt_vm_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_other_disk }}
names={{ ovirt_other_vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
names={{ share_vdo_name }}
logicalsize={{ (((hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_vm_disk }}
names={{ ovirt_vm_vdo_name }}
logicalsize={{ ((hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_other_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_vm_pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_other_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
pvname={{ ovirt_vm_pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}11:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% endif %}

{% elif groups['glusternodes'] | length == 3 %}

# Three nodes detected: simple replica-3 replicated-with-fixed-arbiter volumes

# Active nodes setup

{% set disk_count = [ hostvars[groups['glusternodes'][0]]['hvp_free_disks'] | length, hostvars[groups['glusternodes'][1]]['hvp_free_disks'] | length ] | min %}

{% if disk_count | int == 1 %}
# Only one suitable disk
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
{% set node_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set pv_name = hvp_use_vdo | ternary('mapper/vdo_' + node_disk, node_disk) %}

{% if hvp_use_vdo %}
{% set vdo_name = ('vdo_' + node_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ node_disk }}
names={{ vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int == 2 %}
# Two suitable disks found
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set share_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + share_disk, share_disk) %}
# Note: we will use the smallest for oVirt
{% set ovirt_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}
{% set ovirt_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_disk, ovirt_disk) %}

{% if hvp_use_vdo %}
{% set ovirt_vdo_name = ('vdo_' + ovirt_disk) %}
{% set share_vdo_name = ('vdo_' + share_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_disk }}
names={{ ovirt_vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
names={{ share_vdo_name }}
logicalsize={{ (((hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% elif disk_count | int >= 3 %}
# Three or more suitable disks found
{% for node_index in range(2) %}
# Node {{ node_index }} setup
{% set suitable_disks = hostvars[groups['glusternodes'][node_index]]['hvp_free_disks'] %}
# Note: we will use the first biggest for CTDB/share/backup
{% set share_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | max) | map(attribute='name') | list | sort | first %}
{% set share_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + share_disk, share_disk) %}
# Note: we will use the last smallest for oVirt Engine and ISO
{% set ovirt_other_disk = suitable_disks | selectattr('size', 'equalto', suitable_disks | map(attribute='size') | min) | map(attribute='name') | list | sort | last %}
{% set ovirt_other_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_other_disk, ovirt_other_disk) %}
# Note: we will use the first remaining (excluding those already selected) for oVirt virtual machines
{% set ovirt_vm_disk = suitable_disks | map(attribute='name') | difference([ ovirt_other_disk, share_disk ]) | list | sort | first %}
{% set ovirt_vm_pv_name = hvp_use_vdo | ternary('mapper/vdo_' + ovirt_vm_disk, ovirt_vm_disk) %}

{% if hvp_use_vdo %}
{% set share_vdo_name = ('vdo_' + share_disk) %}
{% set ovirt_other_vdo_name = ('vdo_' + ovirt_other_disk) %}
{% set ovirt_vm_vdo_name = ('vdo_' + ovirt_vm_disk) %}
[vdo{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_other_disk }}
names={{ ovirt_other_vdo_name }}
logicalsize={{ (((hvp_enginedomain_size | regex_replace('[^0-9]*', '') | int) + (hvp_isodomain_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_disk }}
names={{ share_vdo_name }}
logicalsize={{ (((hvp_ctdb_size | regex_replace('[^0-9]*', '') | int) + (hvp_winshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_unixshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_blockshare_size | regex_replace('[^0-9]*', '') | int) + (hvp_backup_size | regex_replace('[^0-9]*', '') | int)) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G

[vdo{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_vm_disk }}
names={{ ovirt_vm_vdo_name }}
logicalsize={{ ((hvp_vmstoredomain_size | regex_replace('[^0-9]*', '') | int) * 1.2) | int }}G
blockmapcachesize=128M
readcache=enabled
readcachesize=20M
emulate512=yes
writepolicy=auto
ignore_vdo_errors=no
slabsize=32G
{% endif %}

[pv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_other_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ share_pv_name }}
ignore_pv_errors=no

[pv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
devices={{ ovirt_vm_pv_name }}
ignore_pv_errors=no

[vg{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
pvname={{ ovirt_other_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
pvname={{ share_pv_name }}
ignore_vg_errors=no

[vg{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
pvname={{ ovirt_vm_pv_name }}
ignore_vg_errors=no

[lv{{ node_index }}1:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv{{ node_index }}2:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thick
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
size={{ hvp_enginedomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}3:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}4:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}5:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk3
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}6:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk1
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_size }}
ignore_lv_errors=no

[lv{{ node_index }}7:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv{{ node_index }}8:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}9:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}10:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_size }}
ignore_lv_errors=no

[lv{{ node_index }}11:{{ groups['glusternodes'][node_index] }}]
action=create
vgname=vgdisk2
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_size }}
ignore_lv_errors=no

{% endfor %}

{% endif %}

# Arbiter node setup

{% set arbiter_disk = hostvars[groups['glusternodes'][2]]['hvp_free_disks'] | selectattr('size', 'equalto', hostvars[groups['glusternodes'][2]]['hvp_free_disks'] | map(attribute='size') | min) | map(attribute='name') | list | sort | first %}

[pv21:{{ groups['glusternodes'][2] }}]
action=create
devices={{ arbiter_disk }}
ignore_pv_errors=no

[vg21:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
pvname={{ arbiter_disk }}
ignore_vg_errors=no

[lv21:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thick
lvname=ctdb
mount=/gluster_bricks/ctdb/brick1
size={{ hvp_ctdb_size }}
ignore_lv_errors=no

[lv22:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinpool
poolname=lvthinpool
chunksize={{ hvp_thinpool_chunksize }}
extent=90%FREE
ignore_lv_errors=no

[lv23:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=enginedomain
mount=/gluster_bricks/enginedomain/brick1
virtualsize={{ hvp_enginedomain_arbitersize }}
ignore_lv_errors=no

[lv24:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=vmstoredomain
mount=/gluster_bricks/vmstoredomain/brick1
virtualsize={{ hvp_vmstoredomain_arbitersize }}
ignore_lv_errors=no

[lv25:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=isodomain
mount=/gluster_bricks/isodomain/brick1
virtualsize={{ hvp_isodomain_arbitersize }}
ignore_lv_errors=no

[lv26:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=winshare
mount=/gluster_bricks/winshare/brick1
virtualsize={{ hvp_winshare_arbitersize }}
ignore_lv_errors=no

[lv27:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=unixshare
mount=/gluster_bricks/unixshare/brick1
virtualsize={{ hvp_unixshare_arbitersize }}
ignore_lv_errors=no

[lv28:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=blockshare
mount=/gluster_bricks/blockshare/brick1
virtualsize={{ hvp_blockshare_arbitersize }}
ignore_lv_errors=no

[lv29:{{ groups['glusternodes'][2] }}]
action=create
vgname=vgarbiter
lvtype=thinlv
poolname=lvthinpool
lvname=backup
mount=/gluster_bricks/backup/brick1
virtualsize={{ hvp_backup_arbitersize }}
ignore_lv_errors=no

{% else %}
# TODO: add support for more than 3 nodes (rotating arbiter - replicated+distributed volumes with a multiple of 3 disks on each node)
# TODO: assume that 3 free disks for each node are equivalent to one disk in the fixed-arbiter configuration above
# TODO: the following will intentionally cause an error - remove when adding complete logic
UNSUPPORTED ({{ groups['glusternodes'] | length }}) NUMBER OF NODES !!!
{% endif %}

[service1]
action=enable
service=chronyd

[service2]
action=restart
service=chronyd

[service3]
action=enable
service=glusterd

[service4]
action=restart
service=glusterd
# Note: we use a custom, more extensive slice setup
slice_setup=no

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp,24010/tcp,3260/tcp
services=glusterfs

# Gluster volume definitions
# TODO: add support for more than 3 nodes (replicated+distributed volumes)
# Note: shard block size lowered from 512 MiB to 64 MiB as per https://bugzilla.redhat.com/show_bug.cgi?id=1468969

[volume1]
action=create
volname={{ hvp_enginedomain_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads (part of group virt settings, so that must be avoided too)
key=storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off,off,off,enable
{% else %}
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads
value=virt,36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8
{% endif %}

brick_dirs=/gluster_bricks/enginedomain/brick1/brick
ignore_volume_errors=no

[volume2]
action=create
volname={{ hvp_vmstoredomain_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads (part of group virt settings, so that must be avoided too)
key=storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off,off,off,enable
{% else %}
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads
value=virt,36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8
{% endif %}

brick_dirs=/gluster_bricks/vmstoredomain/brick1/brick
ignore_volume_errors=no

[volume3]
action=create
volname={{ hvp_isodomain_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes
# TODO: use NFS-Ganesha as soon as a suitable CTDB-based setup has been devised - using internal Gluster-NFS meanwhile

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads (part of group virt settings, so that must be avoided too)
key=storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock,nfs.disable
value=36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off,off,off,enable,off
{% else %}
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,nfs.disable,cluster.shd-max-threads
value=virt,36,36,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off,8
{% endif %}

brick_dirs=/gluster_bricks/isodomain/brick1/brick
ignore_volume_errors=no

[volume4]
action=create
volname={{ hvp_ctdb_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads
key=storage.owner-uid,storage.owner-gid,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,32,full,granular,10000,10,off,on,off,on,on,off,off,off,enable
{% else %}
key=storage.owner-uid,storage.owner-gid,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads,performance.quick-read,performance.read-ahead,performance.io-cache,cluster.eager-lock
value=36,36,32,full,granular,10000,10,off,on,off,on,on,8,off,off,off,enable
{% endif %}

brick_dirs=/gluster_bricks/ctdb/brick1/brick
ignore_volume_errors=no

[volume5]
action=create
volname={{ hvp_winshare_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads
key=group,group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,performance.cache-samba-metadata,features.show-snapshot-directory,performance.readdir-ahead,performance.parallel-readdir
value=metadata-cache,nl-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,on,on,on,on
{% else %}
key=group,group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads,performance.cache-samba-metadata,features.show-snapshot-directory,performance.readdir-ahead,performance.parallel-readdir
value=metadata-cache,nl-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8,on,on,on,on
{% endif %}

brick_dirs=/gluster_bricks/winshare/brick1/brick
ignore_volume_errors=no

[volume6]
action=create
volname={{ hvp_unixshare_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes
# TODO: use NFS-Ganesha as soon as a suitable CTDB-based setup has been devised - using internal Gluster-NFS meanwhile

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads
key=group,group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,nfs.disable,features.show-snapshot-directory,performance.readdir-ahead,performance.parallel-readdir
value=metadata-cache,nl-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off,on,on,on
{% else %}
key=group,group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads,nfs.disable,features.show-snapshot-directory,performance.readdir-ahead,performance.parallel-readdir
value=metadata-cache,nl-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8,off,on,on,on
{% endif %}

brick_dirs=/gluster_bricks/unixshare/brick1/brick
ignore_volume_errors=no

[volume7]
action=create
volname={{ hvp_blockshare_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads (part of group gluster-block settings, so that must be avoided too)
key=storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops
value=0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on
{% else %}
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads
value=gluster-block,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8
{% endif %}

brick_dirs=/gluster_bricks/blockshare/brick1/brick
ignore_volume_errors=no

[volume8]
action=create
volname={{ hvp_backup_volume_name }}
transport=tcp

{% if groups['glusternodes'] | length >= 3 %}
replica=yes
replica_count=3
{% endif %}

{% if groups['glusternodes'] | length == 3 %}arbiter_count=1{% endif %}

force=yes

{% if groups['glusternodes'] | length == 1 %}
# Note: single node does not support cluster.shd-max-threads
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,nfs.disable
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,off
{% else %}
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-wait-qlength,network.ping-timeout,user.cifs,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,cluster.use-compound-fops,cluster.shd-max-threads,nfs.disable
value=metadata-cache,0,0,on,64MB,32,full,granular,10000,10,off,on,off,on,on,8,off
{% endif %}

brick_dirs=/gluster_bricks/backup/brick1/brick
ignore_volume_errors=no
