---
# Ansible playbook to properly startup a whole HVP cluster previously shut down using our playbook
- name: Generate SSH key if not present
  hosts: localhost
  tasks:
    - include_tasks: ../common/tasks/createkeys.yaml
- name: Perform initial poweron on all nodes
  # TODO: physically power on nodes, maybe using BMC access
  hosts: ovirtnodes
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Reset failed status on all units
      command: "systemctl reset-failed"
    - name: Make sure that basic needed services are up and running
      systemd:
        name: "{{ item }}"
        state: started
      with_items:
        - glusterd.service
- name: Start GlusterFS volumes
  hosts: ovirt_master
  remote_user: root
  tasks:
    - name: Start all GlusterFS volumes
      shell: "for volname in $(gluster volume list); do gluster --mode=script volume start ${volname} force ; sleep 5 ; done"
- name: Perform further startup actions on all nodes
  hosts: ovirtnodes
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Restart/enable Gluster-block services
      # TODO: given the problems with Gluster-block, we interpret an empty list of LUNs as a choice for disabling Gluster-block altogether - revert when fixed
      systemd:
        name: "{{ item }}"
        state: restarted
        enabled: yes
      with_items:
        - gluster-block-target.service
        - gluster-blockd.service
      loop_control:
        pause: 5
      when: (hvp_lun_sizes | length) > 0
    - name: Restart/enable CTDB services
      # Note: GlusterFS mount units get automatically started as dependencies
      systemd:
        name: "{{ item.name }}"
        state: "{{ item.state }}"
        enabled: yes
      with_items:
        - { name: 'ctdb.service', state: 'stopped' }
        - { name: 'gluster-lock.mount', state: 'stopped' }
        - { name: 'var-run-gluster-shared_storage.mount', state: 'stopped' }
        - { name: 'ctdb.service', state: 'started' }
      loop_control:
        pause: 10
    - name: Restart/enable oVirt services
      # Note: VDSMD and HA broker units get automatically started as dependencies
      systemd:
        name: "{{ item.name }}"
        state: "{{ item.state }}"
        enabled: yes
      with_items:
        - { name: 'ovirt-ha-agent.service', state: 'stopped' }
        - { name: 'ovirt-ha-broker.service', state: 'stopped' }
        - { name: 'vdsmd.service', state: 'stopped' }
        - { name: 'ovirt-ha-agent.service', state: 'started' }
      loop_control:
        pause: 10
- name: Take oVirt Hosted Engine out of maintenance
  hosts: ovirt_master
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Wait for HA daemons to start responding
      # Note: sometimes immediately checking Engine status gives errors - ignoring errors and waiting
      # Note: we expect to find the global maintenance status as per previous invocation of our own globalshutdown.yaml playbook
      vars:
        ansible_ssh_pipelining: no
      shell: "hosted-engine --vm-status"
      ignore_errors: yes
      retries: 60
      delay: 30
      register: getvalidvmstatus_result
      until: getvalidvmstatus_result.stdout.find('GLOBAL MAINTENANCE') != -1
    - name: Wait for exit from global maintenance
      # Note: sometimes exit from global maintenance requires multiple attempts
      vars:
        ansible_ssh_pipelining: no
      shell: "hosted-engine --set-maintenance --mode=none; sleep 3; hosted-engine --vm-status"
      retries: 60
      delay: 30
      register: exitglobalmaintenance_result
      until: exitglobalmaintenance_result.stdout.find('GLOBAL MAINTENANCE') == -1
    - name: Wait for good Engine health
      vars:
        ansible_ssh_pipelining: no
      shell: "hosted-engine --vm-status | grep -i good"
      retries: 60
      delay: 30
      register: enginehealth_result
      until: enginehealth_result is succeeded
- name: Perform final startup operations through the Engine
  hosts: ovirtengine
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Obtain oVirt Engine SSO token
      no_log: true
      ovirt_auth:
        url: "{{ url }}"
        username: "{{ username }}"
        password: "{{ password }}"
        ca_file: "{{ ca_file }}"
    - name: Put main Datacenter storage domain out of maintenance
      # TODO: wait option notwithstanding, the following would return with storage domain still in maintenance - looping as a workaround - remove when fixed upstream
      ovirt_storage_domain:
        auth: "{{ ovirt_auth }}"
        name: "{{ vmstore_sd_name }}"
        data_center: "{{ dc_name }}"
        state: present
        wait: true
      register: activatevmstoredomain_result
      retries: 60
      delay: 30
      until: activatevmstoredomain_result.storagedomain.status == "active"
    - name: Wait until main Datacenter actually gets operational
      # Note: when main storage domain exits maintenance datacenter goes operational
      ovirt_datacenter_facts:
        auth: "{{ ovirt_auth }}"
        pattern: "name={{ dc_name }}"
      retries: 60
      delay: 30
      until: "ovirt_datacenters[0].status == 'up'"
    - name: Put ISO storage domain out of maintenance
      # Note: the following may fail because of temporary NFS access problems from nodes - retrying to work around
      # TODO: wait option notwithstanding, the following would return with storage domain still in maintenance - looping as a workaround - remove when fixed upstream
      ovirt_storage_domain:
        auth: "{{ ovirt_auth }}"
        name: "{{ iso_sd_name }}"
        data_center: "{{ dc_name }}"
        state: present
        wait: true
      register: activateisodomain_result
      retries: 60
      delay: 30
      until: activateisodomain_result.storagedomain.status == "active"
    - name: Startup all guest VMs
      # Note: ignoring errors to allow for non-existent VMs
      # TODO: check for VM existence and skip if missing
      ovirt_vm:
        auth: "{{ ovirt_auth }}"
        state: running
        cluster: "{{ cluster_name }}"
        name: "{{ item.vm_name }}"
        wait: true
      ignore_errors: yes
      with_items: "{{ guest_vms }}"
    - name: Revoke the SSO token
      no_log: true
      ovirt_auth:
        state: absent
        ovirt_auth: "{{ ovirt_auth }}"
...
