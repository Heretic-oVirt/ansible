---
# Ansible playbook to properly startup a whole HVP cluster previously shut down using our playbook
- name: Generate SSH key if not present
  hosts: localhost
  tasks:
    - include_tasks: ../common/tasks/createkeys.yaml
- name: Perform initial poweron on all nodes
  # TODO: physically power on nodes, maybe using BMC access
  hosts: ovirtnodes
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Make sure that basic needed services are up and running
      systemd:
        name: "{{ item }}"
        state: started
      with_items:
        - glusterd.service
- name: Start GlusterFS volumes
  hosts: ovirt_master
  remote_user: root
  tasks:
    - name: Start all GlusterFS volumes
      shell: "for volname in $(gluster volume list); do gluster --mode=script volume start ${volname} force ; sleep 5 ; done"
- name: Perform further startup actions on all nodes
  hosts: ovirtnodes
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Restart Gluster-block services
      # TODO: given the problems with Gluster-block, we interpret an empty list of LUNs as a choice for disabling Gluster-block altogether - revert when fixed
      systemd:
        name: "{{ item }}"
        state: restarted
      with_items:
        - gluster-block-target.service
        - gluster-blockd.service
      loop_control:
        pause: 5
      when: (hvp_lun_sizes | length) > 0
    - name: Restart CTDB services
      systemd:
        name: "{{ item }}"
        state: restarted
      with_items:
        - gluster-lock.mount
        - ctdb.service
      loop_control:
        pause: 5
    - name: Restart oVirt HA services
      systemd:
        name: "{{ item }}"
        state: restarted
      with_items:
        - ovirt-ha-broker.service
        - ovirt-ha-agent.service
        - vdsmd.service
      loop_control:
        pause: 5
- name: Take oVirt Hosted Engine out of maintenance
  hosts: ovirt_master
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Remove global maintenance
      vars:
        ansible_ssh_pipelining: no
      command: hosted-engine --set-maintenance --mode=none
      register: exitglobalmaintenance_result
    - name: Wait for good Engine health
      vars:
        ansible_ssh_pipelining: no
      shell: "hosted-engine --vm-status | grep -i good"
      retries: 60
      delay: 30
      register: enginehealth_result
      until: enginehealth_result is succeeded
- name: Perform final startup operations through the Engine
  hosts: ovirtengine
  remote_user: root
  tasks:
    - include_tasks: ../common/tasks/setupkeys.yaml
    - name: Get common vars from files
      include_vars:
        file: "../common/vars/hvp.yaml"
    - name: Obtain oVirt Engine SSO token
      no_log: true
      ovirt_auth:
        url: "{{ url }}"
        username: "{{ username }}"
        password: "{{ password }}"
        ca_file: "{{ ca_file }}"
    - name: Put main Datacenter storage domain out of maintenance
      # Note: the following actually returns with storage domain still in maintenance
      # TODO: should wait for storage domain to exit maintenance
      # TODO: unfortunately status doesn't get correctly registered for use with until
      # TODO: unfortunately ovirt_storage_domain_facts does not return status property
      ovirt_storage_domain:
        auth: "{{ ovirt_auth }}"
        name: "{{ vmstore_sd_name }}"
        data_center: "{{ dc_name }}"
        state: present
        wait: true
    - name: Wait until main Datacenter actually gets operational
      # Note: when main storage domain exits maintenance datacenter goes operational
      ovirt_datacenter_facts:
        auth: "{{ ovirt_auth }}"
        pattern: "name={{ dc_name }}"
      retries: 60
      delay: 30
      until: "ovirt_datacenters[0].status == 'up'"
    - name: Put ISO storage domain out of maintenance
      # Note: the following may fail because of temporary NFS access problems from nodes - retrying to work around
      # Note: the following actually returns with storage domain still in maintenance
      # TODO: should wait for storage domain to exit maintenance
      # TODO: unfortunately status doesn't get correctly registered for use with until
      # TODO: unfortunately ovirt_storage_domain_facts does not return status property
      ovirt_storage_domain:
        auth: "{{ ovirt_auth }}"
        name: "{{ iso_sd_name }}"
        data_center: "{{ dc_name }}"
        state: present
        wait: true
      register: isosdenable_result
      retries: 60
      delay: 30
      until: isosdenable_result is succeeded
    - name: Startup all guest VMs
      # Note: ignoring errors to allow for non-existent VMs
      # TODO: check for VM existence and skip if missing
      ovirt_vm:
        auth: "{{ ovirt_auth }}"
        state: running
        cluster: "{{ cluster_name }}"
        name: "{{ item.vm_name }}"
        wait: true
      ignore_errors: true
      with_items: "{{ guest_vms }}"
    - name: Revoke the SSO token
      no_log: true
      ovirt_auth:
        state: absent
        ovirt_auth: "{{ ovirt_auth }}"
...
