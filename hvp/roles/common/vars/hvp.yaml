# Global variables for HVP Ansible playbooks
# Note: this file will be overridden by install-time autodetection logic

# HVP local conventions
hvp_management_domainname: mgmt.private
hvp_gluster_domainname: gluster.private
hvp_storage_name: discord
hvp_orthodox_mode: False
hvp_master_node: "{{ groups['ovirt_master'][0] }}"
# TODO: dynamically determine proper values for Engine RAM/CPUs/imgsize
hvp_engine_ram: 4096
hvp_engine_cpus: 2
hvp_engine_imgsize: 80
hvp_engine_setup_timeout: 7200
hvp_engine_name: celestia
hvp_engine_domainname: "{{ hvp_management_domainname }}"
hvp_engine_ip: "{{ lookup('dig', hvp_engine_name + '.' + hvp_engine_domainname + '.') }}"
hvp_engine_netprefix: 24
# TODO: derive the following by means of Ansible DNS lookup on ovirtnodes names
hvp_engine_dnslist: 172.20.10.10,172.20.10.11,172.20.10.12
# Note: generally, we try to use an independent pingable IP (central managed switch console interface) as "gateway" for oVirt setup
# Note: we do not expect a virtualized setup to have an independent pingable IP apart from the default gateway
hvp_switch_ip: "{% if hostvars[hvp_master_node]['ansible_virtualization_role'] == 'guest' %} 172.20.10.1 {% else %} 172.20.10.200 {% endif %}"
# Note: generally, we keep a distinct proper gateway address on the management network only for network routing configuration
hvp_gateway_ip: 172.20.10.1
hvp_timezone: UTC
hvp_mgmt_bridge_name: ovirtmgmt
hvp_firewall_manager: "firewalld"
hvp_spice_pki_subject: "C=EN, L=Test, O=Test, CN=Test"
hvp_pki_subject: "/C=EN/L=Test/O=Test/CN=Test"
hvp_ca_subject: "/C=EN/L=Test/O=Test/CN=TestCA"
hvp_admin_username: hvpadmin
hvp_email_sender: root@localhost
hvp_email_receiver: monitoring@localhost

## HVP OVN settings
hvp_ovn_private_network_names:
  - dmz0
  - dmz1

# HVP Gluster settings
# TODO: derive proper values for Gluster volume sizes from user settings and/or available space
# TODO: dynamically determine arbiter sizes for each Gluster volume
hvp_enginedomain_volume_name: engine
hvp_enginedomain_size: "{{ (hvp_engine_imgsize * 1.2) | int | abs }}GB"
hvp_enginedomain_arbitersize: "1GB"
hvp_vmstoredomain_volume_name: vmstore
hvp_vmstoredomain_size: "500GB"
hvp_vmstoredomain_arbitersize: "1GB"
hvp_isodomain_volume_name: iso
hvp_isodomain_size: "30GB"
hvp_isodomain_arbitersize: "1GB"
hvp_ctdb_volume_name: ctdb
hvp_ctdb_size: "1GB"
hvp_winshare_volume_name: winshare
hvp_winshare_size: "1024GB"
hvp_winshare_arbitersize: "10GB"
hvp_unixshare_volume_name: unixshare
hvp_unixshare_size: "1024GB"
hvp_unixshare_arbitersize: "10GB"
hvp_blockshare_volume_name: blockshare
hvp_blockshare_size: "1024GB"
hvp_blockshare_arbitersize: "10GB"
hvp_backup_volume_name: backup
hvp_backup_size: "1024GB"
hvp_backup_arbitersize: "10GB"
hvp_thinpool_chunksize: "1536k"

# HVP Gluster-block settings
# TODO: derive proper values for Gluster-block LUN sizes from user settings and/or available space
hvp_lun_sizes:
  - 200GiB
  - 300GiB
  - 450GiB

# Engine credentials:
url: "https://{{ hvp_engine_name }}.{{ hvp_engine_domainname }}/ovirt-engine/api"
username: admin@internal
password: HVP_dem0
ca_file: /etc/pki/ovirt-engine/ca.pem

# Hosts credentials:
# Note: the user must manually confirm BMC settings by editing here
host_password: HVP_dem0
#host_bmc_type: ipmilan
#host_bmc_options: []
host_bmc_user: hvpbmcadmin
host_bmc_password: HVP_dem0

# Env:
## Datacenter:
# TODO: forcing default name since any personalization does not get into appliance cloudinit and causes mismatch - open Bugzilla ticket and revert
hvp_dc_name: HVPDataCenter
dc_name: "Default"
compatibility_version: 4.1

## Cluster:
# TODO: forcing default name since any personalization does not get into appliance cloudinit and causes mismatch - open Bugzilla ticket and revert
hvp_cluster_name: HVPCluster
cluster_name: "Default"

## Storage
# Note: ISO domain will be of type NFS while all others will be of type GlusterFS
# Note: Engine vm has no access to Gluster network, so for ISO domain we must resort to NFS on management network (Engine must access it for image upload)
glusterfs_addr: "{{ groups['gluster_master'] | first }}"
glusterfs_mountopts: "backup-volfile-servers={{ groups['gluster_nonmaster_nodes'] | join(':') }},fetch-attempts=2,log-level=WARNING"
iso_sd_type: nfs
iso_sd_addr: "{{ hvp_storage_name }}.{{ hvp_management_domainname }}"
iso_sd_name: "{{ hvp_isodomain_volume_name + '_domain' }}"
iso_sd_path: "/{{ hvp_isodomain_volume_name }}"
iso_sd_mountopts: 
vmstore_sd_type: glusterfs
vmstore_sd_addr: "{{ glusterfs_addr }}"
vmstore_sd_name: "{{ hvp_vmstoredomain_volume_name + '_domain' }}"
vmstore_sd_path: "/{{ hvp_vmstoredomain_volume_name }}"
vmstore_sd_mountopts: "{{ glusterfs_mountopts }}"
engine_sd_type: glusterfs
engine_sd_addr: "{{ glusterfs_addr }}"
engine_sd_name: "{{ hvp_enginedomain_volume_name + '_domain' }}"
engine_sd_path: "/{{ hvp_enginedomain_volume_name }}"
engine_sd_mountopts: "{{ glusterfs_mountopts }}"

## Networking
got_gluster_network: true
gluster_network: 172.20.11.0/24

got_lan_network: false

got_internal_network: false

