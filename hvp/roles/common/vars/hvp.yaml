# Global variables for HVP Ansible playbooks
# Note: this file will be overridden by install-time autodetection logic

## HVP local conventions
hvp_management_domainname: mgmt.private
hvp_gluster_domainname: gluster.private
hvp_lan_domainname: mgmt.private
hvp_storage_name: discord
hvp_orthodox_mode: false
hvp_ovirt_nightly_mode: false
hvp_use_vdo: false
hvp_upgrade_engine: false
hvp_master_node: "{{ groups['ovirt_master'] | first }}"
# Note: workaround for hang on boot in nested kvm - remove when fixed upstream
hvp_vm_machinetype: "{% if hostvars[hvp_master_node]['ansible_virtualization_role'] == 'guest' %}pc-i440fx-rhel7.2.0{% else %}pc{% endif %}"
# TODO: dynamically determine proper values for Engine RAM/CPUs/imgsize
hvp_engine_ram: 4096
hvp_engine_cpus: 2
hvp_engine_imgsize: 80
hvp_engine_setup_timeout: 7200
hvp_engine_name: celestia
hvp_engine_domainname: "{{ hvp_management_domainname }}"
hvp_engine_ip: "{{ lookup('dig', hvp_engine_name + '.' + hvp_engine_domainname + '.') }}"
hvp_engine_netprefix: "24"
# TODO: derive the following by means of Ansible DNS lookup on ovirtnodes names
hvp_engine_dnslist: 172.20.10.10,172.20.10.11,172.20.10.12
hvp_engine_ntp_sources: "{{ groups['ovirtnodes'] }}"
# Note: generally, we try to use an independent pingable IP (central managed switch console interface) as "gateway" for oVirt setup
# Note: when missing an independent pingable IP (apart from the default gateway) repeat the default gateway IP here
hvp_switch_ip: "172.20.10.200"
# Note: generally, we keep a distinct proper gateway address on the management network only for network routing configuration
hvp_gateway_ip: "172.20.10.1"
hvp_metrics_name: luna
hvp_metrics_domainname: "{{ hvp_management_domainname }}"
hvp_timezone: UTC
hvp_firewall_manager: "firewalld"
hvp_spice_pki_subject: "C=EN, L=Test, O=Test, CN=Test"
hvp_pki_subject: "/C=EN/L=Test/O=Test/CN=Test"
hvp_ca_subject: "/C=EN/L=Test/O=Test/CN=TestCA"
hvp_admin_username: hvpadmin
hvp_admin_password: 'hvpdemo'
hvp_email_sender: root@localhost
hvp_email_receiver: monitoring@localhost
hvp_email_relay: ""
hvp_email_relay_use_smtps: false

## HVP OVN settings
hvp_ovn_private_network_names:
  - dmz0
  - dmz1

## HVP Gluster settings
# TODO: derive proper values for Gluster volume sizes from user settings and/or available space
# TODO: dynamically determine arbiter sizes for each Gluster volume
hvp_enginedomain_volume_name: engine
hvp_enginedomain_size: "{{ (hvp_engine_imgsize * 1.2) | int | abs }}GB"
hvp_enginedomain_arbitersize: "1GB"
hvp_vmstoredomain_volume_name: vmstore
hvp_vmstoredomain_size: "500GB"
hvp_vmstoredomain_arbitersize: "1GB"
hvp_isodomain_volume_name: iso
hvp_isodomain_size: "30GB"
hvp_isodomain_arbitersize: "1GB"
hvp_ctdb_volume_name: ctdb
hvp_ctdb_size: "1GB"
hvp_winshare_volume_name: winshare
hvp_winshare_size: "1024GB"
hvp_winshare_arbitersize: "10GB"
hvp_winshare_subfolders:
  - { path: 'users', owner: 'root', group: 'root', mode: '0751' }
  - { path: 'profiles', owner: 'root', group: 'root', mode: '1751' }
  - { path: 'groups', owner: 'root', group: 'root', mode: '0755' }
  - { path: 'software', owner: 'root', group: 'root', mode: '0755' }
hvp_unixshare_volume_name: unixshare
hvp_unixshare_size: "1024GB"
hvp_unixshare_arbitersize: "10GB"
hvp_unixshare_subfolders:
  - { path: 'homes', owner: 'root', group: 'root', mode: '0711' }
  - { path: 'groups', owner: 'root', group: 'root', mode: '0755' }
  - { path: 'software', owner: 'root', group: 'root', mode: '0755' }
  - { path: 'data', owner: 'root', group: 'root', mode: '0755' }
hvp_blockshare_volume_name: blockshare
hvp_blockshare_size: "1024GB"
hvp_blockshare_arbitersize: "10GB"
hvp_backup_volume_name: backup
hvp_backup_size: "1024GB"
hvp_backup_arbitersize: "10GB"
hvp_thinpool_chunksize: "1536k"

## HVP Gluster-block settings
hvp_lun_sizes:
  - 300GiB

## Engine credentials:
url: "https://{{ hvp_engine_name }}.{{ hvp_engine_domainname }}/ovirt-engine/api"
username: admin@internal
password: 'HVP_dem0'
ca_file: /etc/pki/ovirt-engine/ca.pem

## Hosts credentials:
# Note: the user must manually confirm BMC settings by editing here
# TODO: add support for BMC options
host_password: 'HVP_dem0'
#host_bmc_type: ipmilan
##host_bmc_options: []
#host_bmc_user: hvpbmcadmin
#host_bmc_password: 'HVP_dem0'

# Env:
## Datacenter:
# TODO: forcing default name since any personalization does not get into appliance cloudinit and causes mismatch - open Bugzilla ticket and revert
hvp_dc_name: HVPDataCenter
dc_name: "Default"
# TODO: verify whether "master" is a valid compatibility_version - replace otherwise
compatibility_version: "4.2"

## Cluster:
# TODO: forcing default name since any personalization does not get into appliance cloudinit and causes mismatch - open Bugzilla ticket and revert
hvp_cluster_name: HVPCluster
cluster_name: "Default"

## Storage
# Note: ISO domain will be of type NFS while all others will be of type GlusterFS
# Note: Engine vm has no access to Gluster network, so for ISO domain we must resort to NFS on management network (Engine must access it for image upload)
glusterfs_addr: "{{ groups['gluster_master'] | first }}"
glusterfs_mountopts: "{% if groups['glusternodes'] | length >= 3 %}backup-volfile-servers={{ groups['gluster_nonmaster_nodes'] | join(':') }},{% endif %}fetch-attempts=2,log-level=WARNING"
iso_sd_type: nfs
iso_sd_addr: "{{ hvp_storage_name }}.{{ hvp_management_domainname }}"
iso_sd_name: "{{ hvp_isodomain_volume_name + '_domain' }}"
iso_sd_path: "/{{ hvp_isodomain_volume_name }}"
iso_sd_mountopts: 
vmstore_sd_type: glusterfs
vmstore_sd_addr: "{{ glusterfs_addr }}"
vmstore_sd_name: "{{ hvp_vmstoredomain_volume_name + '_domain' }}"
vmstore_sd_path: "/{{ hvp_vmstoredomain_volume_name }}"
vmstore_sd_mountopts: "{{ glusterfs_mountopts }}"
engine_sd_type: glusterfs
engine_sd_addr: "{{ glusterfs_addr }}"
engine_sd_name: "{{ hvp_enginedomain_volume_name + '_domain' }}"
engine_sd_path: "/{{ hvp_enginedomain_volume_name }}"
engine_sd_mountopts: "{{ glusterfs_mountopts }}"

## Networking
hvp_static_address_block_start: 70

got_mgmt_network: true
mgmt_network: 172.20.10.0/24
hvp_mgmt_bridge_name: ovirtmgmt

got_gluster_network: true
gluster_network: 172.20.11.0/24
# Note: no VM access to Gluster network is allowed, so no bridge is needed

got_lan_network: false
lan_network: ""
hvp_lan_bridge_name: ""

got_internal_network: false
internal_network: ""
hvp_internal_bridge_name: ""

## HVP guest VM settings
vms_network_name: "{{ got_lan_network | ternary(hvp_lan_bridge_name, hvp_mgmt_bridge_name) }}"
vms_network_domainname: "{{ hvp_lan_domainname }}"
vms_network: "{{ got_lan_network | ternary(lan_network, mgmt_network) }}"
# TODO: dynamically extract the following from mirrored kickstart files
guest_vms:
  - { vm_kickstart_file: 'hvp-dc-c7.ks', vm_name: 'DomainController', vm_comment: 'Active Directory Domain Controller', vm_delete_protected: yes, vm_high_availability: false, vm_memory: 2GiB, vm_cpu_cores: 1, vm_cpu_sockets: 1, vm_cpu_shares: 1024, vm_type: 'server', vm_operating_system: 'rhel_7x64', vm_disk_size: 60GiB, vm_network_name: "{{ vms_network_name }}", vm_service_ip: "{{ vms_network | ipaddr('220') | ipaddr('address') }}", vm_service_port: 53 }
  - { vm_kickstart_file: 'hvp-db-c7.ks', vm_name: 'DatabaseServer', vm_comment: 'Database Server', vm_delete_protected: yes, vm_high_availability: false, vm_memory: 4GiB, vm_cpu_cores: 1, vm_cpu_sockets: 1, vm_cpu_shares: 1024, vm_type: 'server', vm_operating_system: 'rhel_7x64', vm_disk_size: 120GiB, vm_network_name: "{{ vms_network_name }}", vm_service_ip: "{{ vms_network | ipaddr('230') | ipaddr('address') }}", vm_service_port: 80 }
  - { vm_kickstart_file: 'hvp-pr-c7.ks', vm_name: 'PrintServer', vm_comment: 'Print Server', vm_delete_protected: yes, vm_high_availability: false, vm_memory: 2GiB, vm_cpu_cores: 1, vm_cpu_sockets: 1, vm_cpu_shares: 1024, vm_type: 'server', vm_operating_system: 'rhel_7x64', vm_disk_size: 80GiB, vm_network_name: "{{ vms_network_name }}", vm_service_ip: "{{ vms_network | ipaddr('190') | ipaddr('address') }}", vm_service_port: 445 }
  - { vm_kickstart_file: 'hvp-vd-c7.ks', vm_name: 'TerminalServer', vm_comment: 'Remote Desktop Server', vm_delete_protected: yes, vm_high_availability: false, vm_memory: 8GiB, vm_cpu_cores: 1, vm_cpu_sockets: 1, vm_cpu_shares: 1024, vm_type: 'server', vm_operating_system: 'rhel_7x64', vm_disk_size: 120GiB, vm_network_name: "{{ vms_network_name }}", vm_service_ip: "{{ vms_network | ipaddr('240') | ipaddr('address') }}", vm_service_port: 22 }

## HVP AD-related settings
hvp_adjoin_domain: ad.mgmt.private
hvp_adjoin_realm: AD.MGMT.PRIVATE
hvp_adjoin_username: adhvpadmin
hvp_adjoin_password: 'HVP_dem0'
hvp_netbios_domainname: AD
hvp_netbios_storagename: DISCORD
hvp_ad_range: 9999-1999999999
hvp_autorid_range: 2000000000-3999999999
hvp_autorid_rangesize: 1000000

## HVP custom Yum repo settings
hvp_repo_baseurl: []
hvp_repo_gpgkey: []
